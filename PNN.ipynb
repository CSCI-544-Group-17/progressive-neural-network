{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "3V6dJxixxrln",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf1aae4f-92a9-4fe6-fb9f-a39256ff6425"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "embeddings = []\n",
        "labels = []\n",
        "\n",
        "with open('/content/drive/MyDrive/t5p_small_embeddings/train/train_0.jsonl', 'r') as f:\n",
        "    for line in f:\n",
        "        data = json.loads(line)\n",
        "        embeddings.append(data['embeddings'][0])\n",
        "        labels.append(data['label'])\n",
        "\n",
        "# Convert lists to tensors\n",
        "embeddings0 = torch.tensor(embeddings)\n",
        "labels0 = torch.tensor(labels)\n",
        "\n",
        "print(embeddings0.size())  # This will show the shape of the embeddings tensor\n",
        "print(labels0.size())     # This will show the shape of the labels tensor\n",
        "\n",
        "embeddings = []\n",
        "labels = []\n",
        "\n",
        "with open('/content/drive/MyDrive/t5p_small_embeddings/train/train_1.jsonl', 'r') as f:\n",
        "    for line in f:\n",
        "        data = json.loads(line)\n",
        "        embeddings.append(data['embeddings'][0])\n",
        "        labels.append(data['label'])\n",
        "\n",
        "# Convert lists to tensors\n",
        "embeddings1 = torch.tensor(embeddings)\n",
        "labels1 = torch.tensor(labels)\n",
        "\n",
        "print(embeddings1.size())  # This will show the shape of the embeddings tensor\n",
        "print(labels1.size())     # This will show the shape of the labels tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TprRL3uZEg8P",
        "outputId": "a355d874-9c76-4c4d-fedf-6108f55d4690"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([7000, 256])\n",
            "torch.Size([7000])\n",
            "torch.Size([1515, 256])\n",
            "torch.Size([1515])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "class InitialColumnProgNN(nn.Module):\n",
        "    def __init__(self, topology, activations):\n",
        "        super(InitialColumnProgNN, self).__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i in range(len(topology) - 1):\n",
        "            self.layers.append(nn.Linear(topology[i], topology[i+1]))\n",
        "        self.activations = activations\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = [x]\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x = layer(x)\n",
        "            # removinf softmax from last layer because of crossentropyloss function - vishal\n",
        "            if i != len(self.activations):\n",
        "              x = self.activations[i](x)\n",
        "            h.append(x)\n",
        "\n",
        "        return h\n",
        "\n",
        "class ExtensibleColumnProgNN(nn.Module):\n",
        "    def __init__(self, topology, activations, prev_columns):\n",
        "        super(ExtensibleColumnProgNN, self).__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.lateral_connections = nn.ModuleList()\n",
        "        for i in range(len(topology) - 1):\n",
        "            self.layers.append(nn.Linear(topology[i], topology[i+1]))\n",
        "            if i > 0:\n",
        "                lateral = [nn.Linear(prev_column.layers[i-1].out_features, topology[i+1], bias=False) for prev_column in prev_columns]\n",
        "                self.lateral_connections.append(nn.ModuleList(lateral))\n",
        "        self.activations = activations\n",
        "        self.prev_columns = prev_columns\n",
        "\n",
        "    def forward(self, x, prev_hs):\n",
        "        h = [x]\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x = layer(x)\n",
        "            if i > 0:\n",
        "                for j, lateral in enumerate(self.lateral_connections[i-1]):\n",
        "                    x += lateral(prev_hs[j][i])\n",
        "\n",
        "            # removinf softmax from last layer because of crossentropyloss function - vishal\n",
        "            if i != len(self.activations):\n",
        "              x = self.activations[i](x)\n",
        "            h.append(x)\n",
        "        return h\n",
        "\n",
        "#todo: add training batch size later\n",
        "def train_column(column, data, target, epochs=50, lr=0.001, prev_hs=[]):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(column.parameters(), lr=lr)\n",
        "    # saved h_values for training lateral connections\n",
        "    h_values = []\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward pass\n",
        "        if len(prev_hs) == 0:\n",
        "          tmp = column(data)\n",
        "        else:\n",
        "          tmp = column(data, prev_hs)\n",
        "\n",
        "        output = tmp[-1] # We're using the final layer's output for training\n",
        "\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss.item()}')\n",
        "\n",
        "        if epoch == epochs - 1:\n",
        "            for h_i in tmp:\n",
        "              h_values.append(h_i.detach().clone()[-1])\n",
        "            # h_values = tmp.to_numpy()[:,-1,:]\n",
        "    return h_values\n",
        "\n",
        "def train_PNN():\n",
        "    # todo: make these two as identical sub-networks\n",
        "    topology1 = [256, 100, 64, 25, 2]\n",
        "    topology2 = [256, 100, 64, 25, 2]\n",
        "    # topology2 = [128, 68, 44, 19, 2]\n",
        "\n",
        "    activations = [F.relu, F.relu, F.relu]\n",
        "\n",
        "    col_0 = InitialColumnProgNN(topology1, activations)\n",
        "    h_0 = train_column(col_0, embeddings0, labels0, epochs=2)\n",
        "    print(len(h_0), len(h_0[0]))\n",
        "    # h_0 = col_0(fake1)\n",
        "\n",
        "    col_1 = ExtensibleColumnProgNN(topology2, activations, [col_0])\n",
        "    h_1 = train_column(col_1, embeddings1, labels1, epochs=2, prev_hs=[h_0])\n",
        "    # h_1 = col_1(fake2, [h_0])\n",
        "\n",
        "    # Make sure the column parameters aren't changing when being used by later columns.\n",
        "    # PyTorch parameters are tensors, so we can use `.eq` and `.all` to verify\n",
        "    th0_before = [param.clone() for param in col_0.parameters()]\n",
        "    th1_before = [param.clone() for param in col_1.parameters()]\n",
        "    #... (and so on for other columns)\n",
        "\n",
        "    # Simulate some kind of training here if desired. For now, it's just forward passes.\n",
        "\n",
        "    th0_after = [param for param in col_0.parameters()]\n",
        "    assert all([torch.eq(before, after).all() for before, after in zip(th0_before, th0_after)])\n",
        "    th1_after = [param for param in col_1.parameters()]\n",
        "    assert all([torch.eq(before, after).all() for before, after in zip(th1_before, th1_after)])\n",
        "    #... (and so on for other columns)\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     train_PNN()\n",
        "\n",
        "# todo: make these two as identical sub-networks\n",
        "topology1 = [256, 100, 64, 25, 2]\n",
        "topology2 = [256, 100, 64, 25, 2]\n",
        "# topology2 = [128, 68, 44, 19, 2]\n",
        "\n",
        "activations = [F.relu, F.relu, F.relu]\n",
        "\n",
        "col_0 = InitialColumnProgNN(topology1, activations)\n",
        "h_0 = train_column(col_0, embeddings0, labels0, epochs=50)\n",
        "# h_0 = col_0(fake1)\n",
        "\n",
        "col_1 = ExtensibleColumnProgNN(topology2, activations, [col_0])\n",
        "h_1 = train_column(col_1, embeddings1, labels1, epochs=50, prev_hs=[h_0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgLE9b8ndwSY",
        "outputId": "de004098-5392-423d-ede4-eae7dfd3cedc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Loss: 0.6938989758491516\n",
            "Epoch 2/50, Loss: 0.6933290958404541\n",
            "Epoch 3/50, Loss: 0.6927840113639832\n",
            "Epoch 4/50, Loss: 0.6922221779823303\n",
            "Epoch 5/50, Loss: 0.6916407942771912\n",
            "Epoch 6/50, Loss: 0.6910565495491028\n",
            "Epoch 7/50, Loss: 0.6904148459434509\n",
            "Epoch 8/50, Loss: 0.6896586418151855\n",
            "Epoch 9/50, Loss: 0.6887884140014648\n",
            "Epoch 10/50, Loss: 0.6878147125244141\n",
            "Epoch 11/50, Loss: 0.6867549419403076\n",
            "Epoch 12/50, Loss: 0.6856104135513306\n",
            "Epoch 13/50, Loss: 0.6843828558921814\n",
            "Epoch 14/50, Loss: 0.6830549240112305\n",
            "Epoch 15/50, Loss: 0.6816108822822571\n",
            "Epoch 16/50, Loss: 0.6800478100776672\n",
            "Epoch 17/50, Loss: 0.6783655881881714\n",
            "Epoch 18/50, Loss: 0.6765665411949158\n",
            "Epoch 19/50, Loss: 0.6746563911437988\n",
            "Epoch 20/50, Loss: 0.6726415157318115\n",
            "Epoch 21/50, Loss: 0.670519232749939\n",
            "Epoch 22/50, Loss: 0.6682757139205933\n",
            "Epoch 23/50, Loss: 0.6659070253372192\n",
            "Epoch 24/50, Loss: 0.663419783115387\n",
            "Epoch 25/50, Loss: 0.6608283519744873\n",
            "Epoch 26/50, Loss: 0.6581307053565979\n",
            "Epoch 27/50, Loss: 0.6553053259849548\n",
            "Epoch 28/50, Loss: 0.6523461937904358\n",
            "Epoch 29/50, Loss: 0.6492556929588318\n",
            "Epoch 30/50, Loss: 0.646032452583313\n",
            "Epoch 31/50, Loss: 0.642673134803772\n",
            "Epoch 32/50, Loss: 0.6391755938529968\n",
            "Epoch 33/50, Loss: 0.6355515718460083\n",
            "Epoch 34/50, Loss: 0.6318221688270569\n",
            "Epoch 35/50, Loss: 0.6280105113983154\n",
            "Epoch 36/50, Loss: 0.6241529583930969\n",
            "Epoch 37/50, Loss: 0.620272696018219\n",
            "Epoch 38/50, Loss: 0.6164023280143738\n",
            "Epoch 39/50, Loss: 0.6125695705413818\n",
            "Epoch 40/50, Loss: 0.6087909936904907\n",
            "Epoch 41/50, Loss: 0.60511714220047\n",
            "Epoch 42/50, Loss: 0.6015799641609192\n",
            "Epoch 43/50, Loss: 0.5982058644294739\n",
            "Epoch 44/50, Loss: 0.5950108766555786\n",
            "Epoch 45/50, Loss: 0.5920118093490601\n",
            "Epoch 46/50, Loss: 0.5892224311828613\n",
            "Epoch 47/50, Loss: 0.5866485238075256\n",
            "Epoch 48/50, Loss: 0.5842859745025635\n",
            "Epoch 49/50, Loss: 0.5821176171302795\n",
            "Epoch 50/50, Loss: 0.5801150798797607\n",
            "Epoch 1/50, Loss: 0.6931390166282654\n",
            "Epoch 2/50, Loss: 0.692852795124054\n",
            "Epoch 3/50, Loss: 0.6925944685935974\n",
            "Epoch 4/50, Loss: 0.6923031210899353\n",
            "Epoch 5/50, Loss: 0.6919578909873962\n",
            "Epoch 6/50, Loss: 0.6915403008460999\n",
            "Epoch 7/50, Loss: 0.6910375952720642\n",
            "Epoch 8/50, Loss: 0.6904328465461731\n",
            "Epoch 9/50, Loss: 0.6897032856941223\n",
            "Epoch 10/50, Loss: 0.6888246536254883\n",
            "Epoch 11/50, Loss: 0.6877850890159607\n",
            "Epoch 12/50, Loss: 0.6865997314453125\n",
            "Epoch 13/50, Loss: 0.6852875351905823\n",
            "Epoch 14/50, Loss: 0.6838619112968445\n",
            "Epoch 15/50, Loss: 0.6823059916496277\n",
            "Epoch 16/50, Loss: 0.6805947422981262\n",
            "Epoch 17/50, Loss: 0.6787018179893494\n",
            "Epoch 18/50, Loss: 0.6766099333763123\n",
            "Epoch 19/50, Loss: 0.6743161678314209\n",
            "Epoch 20/50, Loss: 0.671841025352478\n",
            "Epoch 21/50, Loss: 0.6691831946372986\n",
            "Epoch 22/50, Loss: 0.6663174033164978\n",
            "Epoch 23/50, Loss: 0.6632205843925476\n",
            "Epoch 24/50, Loss: 0.659855842590332\n",
            "Epoch 25/50, Loss: 0.6561686396598816\n",
            "Epoch 26/50, Loss: 0.6521435379981995\n",
            "Epoch 27/50, Loss: 0.647792398929596\n",
            "Epoch 28/50, Loss: 0.6431095600128174\n",
            "Epoch 29/50, Loss: 0.6381151676177979\n",
            "Epoch 30/50, Loss: 0.6328014731407166\n",
            "Epoch 31/50, Loss: 0.6271718740463257\n",
            "Epoch 32/50, Loss: 0.6212447881698608\n",
            "Epoch 33/50, Loss: 0.6150395274162292\n",
            "Epoch 34/50, Loss: 0.608549952507019\n",
            "Epoch 35/50, Loss: 0.6018044352531433\n",
            "Epoch 36/50, Loss: 0.5948338508605957\n",
            "Epoch 37/50, Loss: 0.5877241492271423\n",
            "Epoch 38/50, Loss: 0.5805304646492004\n",
            "Epoch 39/50, Loss: 0.5732889771461487\n",
            "Epoch 40/50, Loss: 0.5660228133201599\n",
            "Epoch 41/50, Loss: 0.5587533116340637\n",
            "Epoch 42/50, Loss: 0.5515165328979492\n",
            "Epoch 43/50, Loss: 0.5443592667579651\n",
            "Epoch 44/50, Loss: 0.5373200178146362\n",
            "Epoch 45/50, Loss: 0.5304373502731323\n",
            "Epoch 46/50, Loss: 0.523712158203125\n",
            "Epoch 47/50, Loss: 0.517194926738739\n",
            "Epoch 48/50, Loss: 0.5109193921089172\n",
            "Epoch 49/50, Loss: 0.5049225687980652\n",
            "Epoch 50/50, Loss: 0.4992113709449768\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = []\n",
        "labels = []\n",
        "\n",
        "with open('/content/drive/MyDrive/t5p_small_embeddings/test/test_0.jsonl', 'r') as f:\n",
        "    for line in f:\n",
        "        data = json.loads(line)\n",
        "        embeddings.append(data['embeddings'][0])\n",
        "        labels.append(data['label'])\n",
        "\n",
        "# Convert lists to tensors\n",
        "test_embeddings0 = torch.tensor(embeddings)\n",
        "test_labels0 = torch.tensor(labels)\n",
        "\n",
        "print(test_embeddings0.size())  # This will show the shape of the embeddings tensor\n",
        "print(test_labels0.size())     # This will show the shape of the labels tensor\n",
        "\n",
        "embeddings = []\n",
        "labels = []\n",
        "\n",
        "with open('/content/drive/MyDrive/t5p_small_embeddings/test/test_1.jsonl', 'r') as f:\n",
        "    for line in f:\n",
        "        data = json.loads(line)\n",
        "        embeddings.append(data['embeddings'][0])\n",
        "        labels.append(data['label'])\n",
        "\n",
        "# Convert lists to tensors\n",
        "test_embeddings1 = torch.tensor(embeddings)\n",
        "test_labels1 = torch.tensor(labels)\n",
        "\n",
        "print(test_embeddings1.size())  # This will show the shape of the embeddings tensor\n",
        "print(test_labels1.size())     # This will show the shape of the labels tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WX1WVq8uoXV",
        "outputId": "50788560-d8d4-438a-91e4-c8c3f8650a01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([883, 256])\n",
            "torch.Size([883])\n",
            "torch.Size([171, 256])\n",
            "torch.Size([171])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation on the test set\n",
        "col_0.eval()  # Set the model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    test_outputs = col_0(test_embeddings0)[-1]\n",
        "    _, predicted = torch.max(test_outputs, 1)\n",
        "    accuracy_simple_rnn = torch.sum(predicted == test_labels0).item() / len(test_labels0)\n",
        "    print(f'\\nTest Accuracy for Class-0 RNN: {accuracy_simple_rnn}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lK5K2Z3vBBCT",
        "outputId": "e6f8abfc-641d-4048-c09c-264ab898d324"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Accuracy for Class-0 RNN: 0.6998867497168743\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation on the test set\n",
        "col_1.eval()  # Set the model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    test_outputs = col_1(test_embeddings1, [h_0])[-1]\n",
        "    _, predicted = torch.max(test_outputs, 1)\n",
        "    accuracy_simple_rnn = torch.sum(predicted == test_labels1).item() / len(test_labels1)\n",
        "    print(f'\\nTest Accuracy for Class-1 RNN: {accuracy_simple_rnn}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pleQE4EBBBL4",
        "outputId": "88f2dbfe-dbf4-469a-d94b-855c194e99e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Accuracy for Class-1 RNN: 0.6842105263157895\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "col_0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBSriWtAhO7u",
        "outputId": "55190445-b4ed-4b3c-d29c-f543a29b5cf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "InitialColumnProgNN(\n",
              "  (layers): ModuleList(\n",
              "    (0): Linear(in_features=256, out_features=100, bias=True)\n",
              "    (1): Linear(in_features=100, out_features=64, bias=True)\n",
              "    (2): Linear(in_features=64, out_features=25, bias=True)\n",
              "    (3): Linear(in_features=25, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "col_1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLJut6KqtOqo",
        "outputId": "94ea99e3-48e2-4998-a215-37bc075a9cc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ExtensibleColumnProgNN(\n",
              "  (layers): ModuleList(\n",
              "    (0): Linear(in_features=256, out_features=100, bias=True)\n",
              "    (1): Linear(in_features=100, out_features=64, bias=True)\n",
              "    (2): Linear(in_features=64, out_features=25, bias=True)\n",
              "    (3): Linear(in_features=25, out_features=2, bias=True)\n",
              "  )\n",
              "  (lateral_connections): ModuleList(\n",
              "    (0): ModuleList(\n",
              "      (0): Linear(in_features=100, out_features=64, bias=False)\n",
              "    )\n",
              "    (1): ModuleList(\n",
              "      (0): Linear(in_features=64, out_features=25, bias=False)\n",
              "    )\n",
              "    (2): ModuleList(\n",
              "      (0): Linear(in_features=25, out_features=2, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    }
  ]
}